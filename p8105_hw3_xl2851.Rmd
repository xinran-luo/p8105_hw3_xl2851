---
title: "p8105_hw3_xl2851"
output: github_document
---
# problem 1
## 1.1 How many aisles are there, and which aisles are the most items ordered from?
```{r}
library(p8105.datasets)
library(tidyr)
library(tidyverse)
data("instacart")

```

```{r}
aisle_count=
instacart %>%
  group_by(aisle) %>%
  summarize(number_of_items=n())%>%
  arrange(desc(number_of_items))
```

There are 134 aisles. Fresh vegetables are the most items ordered from.

## 1.2 Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.
```{r}
plot_aisle =
  aisle_count %>%
  filter(number_of_items>10000)%>%
  ggplot(aes(x=number_of_items, y=aisle))+
   labs(
    title = "Aisles plot",
    x = "names of aisles",
    y = "number of items ordered in each aisle",
    caption = "Data from instacart")

plot_aisle+geom_point()
```

## 1.3 Make a table showing the three most popular items in each of the aisles
```{r}
  instacart %>%
  filter(aisle == c("baking ingredients","dog food care","packaged vegetables fruits"))%>%
  group_by(aisle)%>%
  count(product_name, name="num_of_items")%>%
  arrange(desc(num_of_items))%>%
  top_n(3)%>%
  knitr::kable()
```

## 1.4 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r}
  instacart %>%
  filter(product_name==c("Pink Lady Apples","Coffee Ice Cream"))%>%
  group_by(order_dow)%>%
  summarize(
    mean=mean(order_hour_of_day)
  )%>%
  knitr::kable(digits=1)
```

According to the table, the lastest mean onder hour(14.5) is on Wednesday.

There are `r nrow(instacart)` rows and `r ncol(instacart)` columns in data 'instacart'.
There are `r nrow(distinct(instacart, aisle))` distinct aisles. 
Some key variables include (add later)

# problem 2
## clean the data
```{r}
data("brfss_smart2010")
tidied_brfss=
  filter(brfss_smart2010,Topic=="Overall Health")%>%
  janitor::clean_names() %>%
  rename(state = "locationabbr", location = "locationdesc")%>%
  mutate(
    response=as.factor(response),
    response = forcats::fct_relevel(response, c("Poor","Fair","Good", "Very good","Excellent")))

```

## In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
  tidied_brfss%>%
  group_by(state,year)%>%
  summarise(n_location=n_distinct(location, na.rm = FALSE))%>%
  filter(n_location>=7, year==2002)%>%
  knitr::kable()

  tidied_brfss%>%
  group_by(state,year)%>%
  summarise(n_location=n_distinct(location, na.rm = FALSE))%>%
  filter(n_location>=7, year==2010)%>%
  knitr::kable()
```

According to the table, in 2002, CT, FL, MA, NC, NJ and PA were observed at 7 or more locations. In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX and WA were observed at 7 or more locations.

## Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

```{r}
brfss_new =
  tidied_brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(year, location) %>% 
  mutate(mean = mean(data_value, na.rm = FALSE)) %>%
  select(year, location, mean) %>% 
  distinct() 
```

## Make a “spaghetti” plot 
```{r}
ggplot(brfss_new, aes(x = year, y = mean, color = location, group = location))+
  geom_line() +
  theme(legend.position="none") +
  xlab("Year") +
  ylab("Mean value") +
  ggtitle("The average value over time within a state")
```

The plots shows the average data_value across location within a state with "year" as the x-axis and "mean value" as the y-axis.

## two-panel plot
```{r}
  tidied_brfss %>%
  filter(state=="NY",year %in% c(2006, 2010))%>%
  ggplot(aes(x = response, y = data_value, color=response)) +
  geom_boxplot(alpha = .5)+
  facet_grid(~year)+
  theme(text = element_text(size=12),
         axis.text.x = element_text(angle=90, vjust=0.5)) +
  labs(
    title = "Distribution of data_value for responses among locations in NY State",
    x = "Response",
    y = "data_value")
```

The two-panel plot shows for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State. Within these 2 years, the distribution of response is similar but in 2010 there were more "very good" responses.

# problem 3
```{r}
accel_data=read_csv("./accel_data.csv")%>%
   janitor::clean_names() %>%
  mutate(
    type = case_when(
      day %in% c("Saturday", "Sunday") ~ "weekend",
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday", TRUE~""),
    day = as.factor(day),
    day = forcats::fct_relevel(day, c("Sunday","Monday","Tuesday","Wednesday","Thursday", "Friday", "Saturday"))) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix = "activity_",
    values_to = "count"
  )
```

The dataset, "accel_data", has `r nrow(accel_data)` observations and `r ncol(accel_data)` variables. Key variables are week, type, day activity and count.

## aggregate accross minutes to create a total activity variable for each day
```{r}
accel_data %>% 
  group_by(week,day, day_id) %>%
  summarize(activity_total = sum(count)) %>% 
  knitr::kable()
```

I did not see any apparent trends.

## Make a single-panel plot that shows the 24-hour activity time courses for each day 
```{r}
  accel_data %>% 
  group_by(week,day, day_id) %>%
  summarize(activity_total = sum(count))%>%
  rowid_to_column("ID")%>%
    ggplot(aes(x = ID, y = activity_total)) +
    geom_point(aes(color = day), alpha = .5) +
    geom_smooth(se = FALSE)+
labs(
    title = "24-hour activity time courses for each day",
    x = "ID",
    y = "activity_total")
```